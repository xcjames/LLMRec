/home/users/ntu/chen035/.conda/envs/LC-Rec/lib/python3.11/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:38<00:38, 38.08s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:55<00:00, 25.71s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:55<00:00, 27.56s/it]
df: /home/users/ntu/chen035/.triton/autotune: No such file or directory
Traceback (most recent call last):
  File "/scratch/users/ntu/chen035/LC-Rec/finetune.py", line 122, in <module>
    train(args)
  File "/scratch/users/ntu/chen035/LC-Rec/finetune.py", line 71, in train
    args=transformers.TrainingArguments(
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 121, in __init__
  File "/home/users/ntu/chen035/.conda/envs/LC-Rec/lib/python3.11/site-packages/transformers/training_args.py", line 1483, in __post_init__
    and (self.device.type != "cuda")
         ^^^^^^^^^^^
  File "/home/users/ntu/chen035/.conda/envs/LC-Rec/lib/python3.11/site-packages/transformers/training_args.py", line 1921, in device
    return self._setup_devices
           ^^^^^^^^^^^^^^^^^^^
  File "/home/users/ntu/chen035/.conda/envs/LC-Rec/lib/python3.11/site-packages/transformers/utils/generic.py", line 54, in __get__
    cached = self.fget(obj)
             ^^^^^^^^^^^^^^
  File "/home/users/ntu/chen035/.conda/envs/LC-Rec/lib/python3.11/site-packages/transformers/training_args.py", line 1853, in _setup_devices
    self.distributed_state = PartialState(timeout=timedelta(seconds=self.ddp_timeout))
                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/users/ntu/chen035/.conda/envs/LC-Rec/lib/python3.11/site-packages/accelerate/state.py", line 202, in __init__
    from deepspeed import comm as dist
  File "/home/users/ntu/chen035/.conda/envs/LC-Rec/lib/python3.11/site-packages/deepspeed/__init__.py", line 25, in <module>
    from . import ops
  File "/home/users/ntu/chen035/.conda/envs/LC-Rec/lib/python3.11/site-packages/deepspeed/ops/__init__.py", line 15, in <module>
    from ..git_version_info import compatible_ops as __compatible_ops__
  File "/home/users/ntu/chen035/.conda/envs/LC-Rec/lib/python3.11/site-packages/deepspeed/git_version_info.py", line 29, in <module>
    op_compatible = builder.is_compatible()
                    ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/users/ntu/chen035/.conda/envs/LC-Rec/lib/python3.11/site-packages/deepspeed/ops/op_builder/fp_quantizer.py", line 35, in is_compatible
    sys_cuda_major, _ = installed_cuda_version()
                        ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/users/ntu/chen035/.conda/envs/LC-Rec/lib/python3.11/site-packages/deepspeed/ops/op_builder/builder.py", line 51, in installed_cuda_version
    raise MissingCUDAException("CUDA_HOME does not exist, unable to compile CUDA op(s)")
deepspeed.ops.op_builder.builder.MissingCUDAException: CUDA_HOME does not exist, unable to compile CUDA op(s)
E0227 21:15:24.608000 250379 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 250705) of binary: /home/users/ntu/chen035/.conda/envs/LC-Rec/bin/python3.11
Traceback (most recent call last):
  File "/home/users/ntu/chen035/.conda/envs/LC-Rec/bin/torchrun", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/home/users/ntu/chen035/.conda/envs/LC-Rec/lib/python3.11/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 355, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/users/ntu/chen035/.conda/envs/LC-Rec/lib/python3.11/site-packages/torch/distributed/run.py", line 918, in main
    run(args)
  File "/home/users/ntu/chen035/.conda/envs/LC-Rec/lib/python3.11/site-packages/torch/distributed/run.py", line 909, in run
    elastic_launch(
  File "/home/users/ntu/chen035/.conda/envs/LC-Rec/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 138, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/users/ntu/chen035/.conda/envs/LC-Rec/lib/python3.11/site-packages/torch/distributed/launcher/api.py", line 269, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
finetune.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-02-27_21:15:24
  host      : x1000c0s0b0n1.hostmgmt2000.cm.asp2a.nscc.sg
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 250705)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
